# Chat with Your Code â€“ RAG for C/C++ Projects  

ğŸš€ **Chat with Your Code** is an AI-powered system that allows developers to ask natural language questions about a C/C++ codebase and receive meaningful, context-aware explanations.  
It combines **Retrieval-Augmented Generation (RAG)** with **LLM-based reasoning** to make navigating large, complex codebases easier.  

---

## âœ¨ Features  

- ğŸ“‚ **Codebase Ingestion** â€“ Parses `.c`, `.cpp`, and `.h` files into semantically meaningful chunks (functions, comments, docstrings).  
- ğŸ” **Semantic Search** â€“ Uses embeddings + vector DB (ChromaDB) to retrieve relevant code snippets.  
- ğŸ¤– **AI-Powered Q&A** â€“ LLM answers questions strictly grounded in retrieved context.  
- ğŸ“ **Detailed Explanations** â€“ Provides line-by-line or logical flow breakdown of functions.  
- ğŸ¨ **Mermaid Diagrams** â€“ Automatically generates flowcharts for functions to visualize execution flow.  
- ğŸ’» **Interactive Frontend** â€“ Streamlit app with query box, retrieved code snippets, and rendered diagrams.  
- ğŸŒ³ **Semantic Chunking with AST** â€“ Instead of naive text-splitting, leverages **tree-sitter** to parse C/C++ into an Abstract Syntax Tree (AST). Chunks are based on logical syntax units (functions, classes, structs, comments).  
- ğŸ·ï¸ **Rich Metadata Extraction** â€“ Each chunk is enriched with metadata such as `type`, `function_name`, `class_name`, `struct_name`, and `start_line`, stored in ChromaDB for precise retrieval.  
- âš¡ **Efficient Vector Retrieval** â€“ Natural language queries are embedded via SentenceTransformer and matched against stored embeddings in ChromaDB for accurate similarity search.  
- ğŸ”„ **Robust Ingestion Pipeline** â€“ Handles new source files with batching, unique chunk IDs, metadata enrichment, and efficient embedding storage in ChromaDB.  


---

## ğŸ—ï¸ System Architecture  

```mermaid
flowchart TD
    A[User Query] --> B[FastAPI Backend /ask Endpoint]
    B --> C[RAG Module: Retrieve Relevant Code Chunks]
    C --> D[LLM Module: Gemini 1.5 Flash / GPT]
    D --> E[Answer + Mermaid Diagram]
    E --> F[Streamlit Frontend]
    F --> G[Display: Text + Retrieved Code + Diagram]

---

## ğŸ› ï¸ Tech Stack  

| Technology                     | Purpose                                                                 |
|--------------------------------|-------------------------------------------------------------------------|
| **Python**                     | Core programming language for backend logic                            |
| **FastAPI**                    | Backend framework exposing the `/ask` API endpoint                     |
| **React.js**                   | Frontend framework for building the interactive UI                     |
| **tree-sitter**                | Parser that generates AST for C/C++ to enable semantic chunking         |
| **tree-sitter-languages**      | Provides precompiled grammars for parsing C and C++ code                |
| **SentenceTransformer (all-MiniLM-L6-v2)** | Creates vector embeddings for code chunks & queries |
| **ChromaDB**                   | Lightweight vector database for storing embeddings + metadata           |
| **Mermaid.js**                 | Renders diagrams (flowcharts) in the frontend                          |
| **dotenv**                     | Manages environment variables (API keys, configs)                      |
| **Uvicorn**                    | ASGI server to run FastAPI backend                                     |

---



## ğŸ“‚ Project Structure  

```bash
chat-with-code/
â”‚â”€â”€ backend/                 
â”‚   â”‚â”€â”€ main.py              # FastAPI backend with /ask endpoint
â”‚   â”‚â”€â”€ rag_module.py        # Vector DB (ChromaDB) + C/C++ function & comment chunking logic + retrieval logic
â”‚   â”‚â”€â”€ llm_module.py        # Gemini/GPT integration with strict system prompt
â”‚
â”‚â”€â”€ frontend/                # react-based UI              
â”‚â”€â”€ data/                    # Sample C/C++ codebases
â”‚â”€â”€ requirements.txt         # Python dependencies
â”‚â”€â”€ README.md                # Project documentation

---

##âš™ï¸ Installation
'''bash
# 1. Clone the repo
git clone https://github.com/your-username/chat-with-code.git
cd chat-with-code

# 2. Create virtual environment & install dependencies
python -m venv venv
source venv/bin/activate   # (Linux/Mac)
venv\Scripts\activate      # (Windows)
pip install -r requirements.txt
'''

**Create a .env file in the root directory:**
'''bash
GEMINI_API_KEY=your_google_gemini_key
'''

**Run the backend:**
'''bash
uvicorn backend.main:app --reload
'''

**Run Frontend:**
'''bash
npm run dev
'''
---



